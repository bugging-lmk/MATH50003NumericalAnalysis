
\section{Gaussian Quadrature}
We have already seen examples of quadrature including the Rectangular, Trapezium and Interpolatory Quadrature Rules. In this section we see that a special type of interpolatory quadrature can be constructed by using the roots of orthogonal polynomials, leading to a method that is exact for polynomials of twice the expected degree. Importantly, we can use quadrature to compute expansions in orthogonal polynomials that interpolate,  mirroring the link between the Trapezium rule, Fourier series, and interpolation but now for orthogonal polynomials.

We begin with a simple example demonstrating the power of using roots of orthogonal polynomials in an interpolatory quadrature rule:

\begin{example}[Gauss-Chebyshev] We  find the interpolatory quadrature rule for $w(x) = 1/\sqrt{1-x^2}$ on $[-1,1]$ with points equal to the roots of $T_3(x)$.  This is a special case of Gaussian quadrature which we will approach in another way below. We use:
\[
\int_{-1}^1 w(x) {\rm d}x = \ensuremath{\pi}, \int_{-1}^1 xw(x) {\rm d}x = 0, \int_{-1}^1 x^2 w(x) {\rm d}x = {\ensuremath{\pi} \over 2}.
\]
From the 3-term recurrence we deduce
\[
T_0(x) = 1, T_1(x) =x, T_2(x) = 2x T_1(x) - T_0(x) = 2x^2-1, T_3(x) = 2x T_2(x) - T_1(x) = 4x^3-3x
\]
hence we find the 3 roots of $T_3(x)$ are $x_1,x_2,x_3 = \sqrt{3}/2,0,-\sqrt{3}/2$. Thus we have:
\begin{align*}
w_1 = \int_{-1}^1 w(x) \ensuremath{\ell}_1(x) {\rm d}x = \int_{-1}^1 {x(x+\sqrt{3}/2) \over (\sqrt{3}/2) \sqrt{3} \sqrt{1-x^2}}{\rm d}x = {\ensuremath{\pi} \over 3} \\
w_2 = \int_{-1}^1 w(x) \ensuremath{\ell}_2(x) {\rm d}x = \int_{-1}^1 {(x-\sqrt{3}/2)(x+\sqrt{3}/2) \over (-3/4)\sqrt{1-x^2}}{\rm d}x = {\ensuremath{\pi} \over 3} \\
w_3 = \int_{-1}^1 w(x) \ensuremath{\ell}_3(x) {\rm d}x = \int_{-1}^1 {(x-\sqrt{3}/2) x \over (-\sqrt{3})(-\sqrt{3}/2) \sqrt{1-x^2}}{\rm d}x = {\ensuremath{\pi} \over 3}
\end{align*}
(It's not a coincidence that they are all the same but this will differ for roots of other OPs.)  That is we have
\[
\ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[f]  = {\ensuremath{\pi} \over 3}\br[ f(\sqrt{3}/2) + f(0) + f(-\sqrt{3}/2) ].
\]
This is indeed exact for polynomials up to degree $n-1=2$, but it goes all the way up to $2n-1 = 5$:
\begin{align*}
\ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[1] &= \ensuremath{\pi}, \ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[x] = 0, \ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[x^2] = {\ensuremath{\pi} \over 2}, \\
\ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[x^3] &= 0, \ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[x^4] &= {3 \ensuremath{\pi} \over 8}, \ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[x^5] = 0 \\
\ensuremath{\Sigma}_n^{w,\ensuremath{\bm{\x}}}[x^6] &= {9 \ensuremath{\pi} \over 32} \ensuremath{\neq} {5 \ensuremath{\pi} \over 16}
\end{align*}
We shall explain this miracle in the rest of this section. \end{example}

\subsection{Roots of orthogonal polynomials and truncated Jacobi matrices}
Consider roots (zeros) of orthogonal polynomials $p_n(x)$ which will be essential to constructing Gaussian quadrature, via interpolation at these points. For interpolation to be well-defined we first need to guarantee that the roots are distinct.

\begin{lemma}[OP roots] An orthogonal polynomial $p_n(x)$ has exactly $n$ distinct roots.

\end{lemma}
\textbf{Proof}

Suppose $x_1, \ensuremath{\ldots},x_j$ are the roots where $p_n(x)$ changes sign, i.e., the order of the root must be odd and hence
\[
p_n(x) = c_k (x-x_k)^{2p+1} + O((x-x_k)^{2p+2})
\]
for $c_k \ensuremath{\neq} 0$ and $k = 1,\ensuremath{\ldots},j$ and $p \ensuremath{\in} \ensuremath{\bbZ}$, as $x \ensuremath{\rightarrow} x_k$. Then
\[
p_n(x) (x-x_1) \ensuremath{\cdots}(x-x_j)
\]
does not change signs: it behaves like $c_k (x-x_k)^{2p+2} + O(x-x_k)^{2p+3}$ as $x \ensuremath{\rightarrow} x_k$. In other words:
\[
\ensuremath{\langle}p_n,(x-x_1) \ensuremath{\cdots}(x-x_j) \ensuremath{\rangle} = \int_a^b p_n(x) (x-x_1) \ensuremath{\cdots}(x-x_j) w(x) {\rm d} x \ensuremath{\neq} 0.
\]
where $w(x)$ is the weight of orthogonality. This is only possible if $j = n$ as $p_n(x)$ is orthogonal w.r.t. all lower degree polynomials and hence otherwise this integral would be zero. Since $p_n(x)$ is exactly degree $n$ it follows each root must be first order and hence distinct.

\ensuremath{\QED}

We can relate these roots to eigenvalues of truncations of Jacobi matrices:

\begin{definition}[truncated Jacobi matrix] Given a Jacobi matrix $J$ associated with a family of orthonormal polynomials,  the \emph{truncated Jacobi matrix} is
\[
J_n := \begin{bmatrix} a_0 & b_0 \\
                         b_0 & \ensuremath{\ddots} & \ensuremath{\ddots} \\
                         & \ensuremath{\ddots} & a_{n-2} & b_{n-2} \\
                         && b_{n-2} & a_{n-1} \end{bmatrix} \ensuremath{\in} \ensuremath{\bbR}^{n \ensuremath{\times} n}
\]
\end{definition}

\begin{lemma}[OP roots and Jacobi matrices] The zeros $x_1, \ensuremath{\ldots},x_n$ of an orthonormal polynomial $q_n(x)$ are the eigenvalues of the truncated Jacobi matrix $J_n$. More precisely,
\[
J_n Q_n = Q_n \begin{bmatrix} x_1 \\ & \ensuremath{\ddots} \\ && x_n \end{bmatrix}
\]
for the orthogonal matrix
\[
Q_n = \underbrace{\begin{bmatrix}
q_0(x_1) & \ensuremath{\cdots} & q_0(x_n) \\
\ensuremath{\vdots}  & \ensuremath{\cdots} & \ensuremath{\vdots}  \\
q_{n-1}(x_1) & \ensuremath{\cdots} & q_{n-1}(x_n)
\end{bmatrix}}_{V_n^\ensuremath{\top}} \begin{bmatrix} \ensuremath{\alpha}_1^{-1} \\ & \ensuremath{\ddots} \\ && \ensuremath{\alpha}_n^{-1} \end{bmatrix}
\]
where $\ensuremath{\alpha}_j = \sqrt{q_0(x_j)^2 + \ensuremath{\cdots} + q_{n-1}(x_j)^2}$.

\end{lemma}
\textbf{Proof}

We construct the eigenvector (noting $b_{n-1} q_n(x_j) = 0$):
\[
J_n \begin{bmatrix} q_0(x_j) \\ \ensuremath{\vdots} \\ q_{n-1}(x_j) \end{bmatrix} =
\begin{bmatrix} a_0 q_0(x_j) + b_0 q_1(x_j) \\
 b_0 q_0(x_j) + a_1 q_1(x_j) + b_1 q_2(x_j) \\
\ensuremath{\vdots} \\
b_{n-3} q_{n-3}(x_j) + a_{n-2} q_{n-2}(x_j) + b_{n-2} q_{n-1}(x_j) \\
b_{n-2} q_{n-2}(x_j) + a_{n-1} q_{n-1}(x_j) + b_{n-1} q_n(x_j)
\end{bmatrix} = x_j \begin{bmatrix} q_0(x_j) \\
 q_1(x_j) \\
\ensuremath{\vdots} \\
q_{n-1}(x_j)
\end{bmatrix}
\]
The spectral theorem guarantees that all symmetric matrices have an orthogonal eigenvector matrix.  That is, by scaling the columns of the eigenvectors we know there must exist $\ensuremath{\alpha}_j$ so that 
\[
Q_n = \underbrace{\begin{bmatrix}
q_0(x_1) & \ensuremath{\cdots} & q_0(x_n) \\
\ensuremath{\vdots}  & \ensuremath{\cdots} & \ensuremath{\vdots}  \\
q_{n-1}(x_1) & \ensuremath{\cdots} & q_{n-1}(x_n)
\end{bmatrix}}_{V_n^\ensuremath{\top}} \begin{bmatrix} \ensuremath{\alpha}_1^{-1} \\ & \ensuremath{\ddots} \\ && \ensuremath{\alpha}_n^{-1} \end{bmatrix}
\]
is orthogonal. We choose $\ensuremath{\alpha}_j$ so that
\[
\ensuremath{\bm{\e}}_j^\ensuremath{\top} Q_n^\ensuremath{\top} Q_n \ensuremath{\bm{\e}}_j = {\ensuremath{\sum}_{k=0}^{n-1} q_k(x_j)^2 \over \ensuremath{\alpha}_j^2} = 1.
\]
\ensuremath{\QED}

\begin{example}[Chebyshev roots] Consider $T_n(x) = \cos n {\rm acos}\, x$. The roots  are $x_j = \cos \ensuremath{\theta}_j$ where $\ensuremath{\theta}_j = (j-1/2)\ensuremath{\pi}/n$ for $j = 1,\ensuremath{\ldots},n$ are the roots of $\cos n \ensuremath{\theta}$ that are inside $[0,\ensuremath{\pi}]$. 

Consider the $n = 3$ case where we have
\[
x_1,x_2,x_3 = \cos(\ensuremath{\pi}/6),\cos(\ensuremath{\pi}/2),\cos(5\ensuremath{\pi}/6) = \sqrt{3}/2,0,-\sqrt{3}/2
\]
We also have from the 3-term recurrence:
\meeq{
T_0(x) = 1, \ccr
T_1(x) = x, \ccr
T_2(x) = 2x T_1(x) - T_0(x) = 2x^2-1, \ccr
T_3(x) = 2x T_2(x) - T_1(x) = 4x^3-3x.
}
As determined as part of the problem sheet, we orthonormalise by rescaling
\begin{align*}
q_0(x) &= 1/\sqrt{\ensuremath{\pi}}, \\
q_k(x) &= T_k(x) \sqrt{2}/\sqrt{\ensuremath{\pi}}.
\end{align*}
so that the Jacobi matrix is symmetric:
\[
x [q_0(x)|q_1(x)|\ensuremath{\cdots}] = [q_0(x)|q_1(x)|\ensuremath{\cdots}] \underbrace{\begin{bmatrix} 0 & 1/\sqrt{2} \\
                            1/\sqrt{2} & 0 & 1/2 \\
                            &1/2 & 0 & 1/2 \\
                             &   & 1/2 & 0 & \ensuremath{\ddots} \\
                              &  && \ensuremath{\ddots} & \ensuremath{\ddots}
\end{bmatrix}}_J
\]
We can then confirm that we have constructed an eigenvector/eigenvalue of the $3 \ensuremath{\times} 3$ truncation of the Jacobi matrix, e.g. at $x_2 = 0$:
\[
\begin{bmatrix} 
0 & 1/\sqrt{2} \\
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} q_0(0) \\ q_1(0) \\ q_2(0) 
    \end{bmatrix} = {1 \over \sqrt \ensuremath{\pi}} \begin{bmatrix} 
0 & 1/\sqrt{2} \\
1/\sqrt{2} & 0 & 1/2 \\
    & 1/2 & 0\end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ -{\sqrt{2}}
    \end{bmatrix} =\begin{bmatrix} 0 \\ 0 \\ 0
    \end{bmatrix}.
\]
\end{example}

\subsection{Properties of Gaussian quadrature}
We now introduce Gaussian quadrature, which we shall see is exact for polynomials up to degree $2n-1$, i.e., double the degree of other interpolatory quadrature rules from other grids.

Rather than defining Gaussian quadrature as an interpolatory quadrature rule, we build an analogy with the discrete Fourier transform (DFT) by defining a quadrature rule for which our orthogonal polynomials satisfy a discrete orthogonality property.

\begin{definition}[Gaussian quadrature] Given a weight $w(x)$, the Gauss quadrature rule is:
\[
\ensuremath{\int}_a^b f(x)w(x) {\rm d}x \ensuremath{\approx} \underbrace{\ensuremath{\sum}_{j=1}^n w_j f(x_j)}_{\ensuremath{\Sigma}_n^w[f]}
\]
where $x_1,\ensuremath{\ldots},x_n$ are the roots of the orthonormal polynomials $q_n(x)$ and 
\[
w_j := {1 \over \ensuremath{\alpha}_j^2} = {1 \over q_0(x_j)^2 + \ensuremath{\cdots} + q_{n-1}(x_j)^2}.
\]
Equivalentally, $x_1,\ensuremath{\ldots},x_n$ are the eigenvalues of $J_n$ and $w_j$ can be written in terms of the eigenvectors and the integral of the weight:
\[
w_j = \ensuremath{\int}_a^b w(x) {\rm d}x \underbrace{q_0(x_j)^2/\ensuremath{\alpha}_j^2}_{Q_n[1,j]^2}.
\]
\end{definition}

In analogy to how Fourier series are orthogonal with respect to the Trapezium rule, Orthogonal polynomials are orthogonal with respect to Gaussian quadrature:

\begin{lemma}[Discrete orthogonality] For $0 \ensuremath{\leq} \ensuremath{\ell},m \ensuremath{\leq} n-1$, the orthonormal polynomials $q_n(x)$ satisfy
\[
\ensuremath{\Sigma}_n^w[q_\ensuremath{\ell} q_m] = \ensuremath{\delta}_{\ensuremath{\ell}m}
\]
\end{lemma}
\textbf{Proof}
\[
\ensuremath{\Sigma}_n^w[q_\ensuremath{\ell} q_m] = \ensuremath{\sum}_{j=1}^n {q_\ensuremath{\ell}(x_j) q_m(x_j) \over \ensuremath{\alpha}_j^2}
= \left[{q_\ensuremath{\ell}(x_1)\over \ensuremath{\alpha}_1} | \ensuremath{\cdots} | {q_\ensuremath{\ell}(x_n) \over \ensuremath{\alpha}_n}\right] 
\begin{bmatrix}
q_m(x_1)/\ensuremath{\alpha}_1 \\
\ensuremath{\vdots} \\
q_m(x_n)/\ensuremath{\alpha}_n \end{bmatrix} = \ensuremath{\bm{\e}}_\ensuremath{\ell}^\ensuremath{\top} Q_n Q_n^\ensuremath{\top} \ensuremath{\bm{\e}}_m = \ensuremath{\delta}_{\ensuremath{\ell}m}
\]
\ensuremath{\QED}

Just as approximating Fourier coefficients using Trapezium rule gives a way of interpolating at the grid, so does Gaussian quadrature:

\begin{theorem}[interpolation via quadrature] For the orthonormal polynomials $q_n(x)$,
\[
f_n(x) := \ensuremath{\sum}_{k=0}^{n-1} c_k^n q_k(x)\hbox{ for } c_k^n := \ensuremath{\Sigma}_n^w[f q_k]
\]
interpolates $f(x)$ at the Gaussian quadrature points $x_1,\ensuremath{\ldots},x_n$.

\end{theorem}
\textbf{Proof} Consider the Vandermonde-like matrix from above:
\[
V_n := \begin{bmatrix} q_0(x_1) & \ensuremath{\cdots} & q_{n-1}(x_1) \\
                \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                q_0(x_n) & \ensuremath{\cdots} & q_{n-1}(x_n) \end{bmatrix}
\]
and define
\[
Q_n^w := V_n^\ensuremath{\top} \begin{bmatrix} w_1 \\ &\ensuremath{\ddots} \\&& w_n \end{bmatrix} = \begin{bmatrix} q_0(x_1)w_1 & \ensuremath{\cdots} &  q_0(x_n) w_n \\
                \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                q_{n-1}(x_1) w_1 & \ensuremath{\cdots} & q_{n-1}(x_n)w_n \end{bmatrix}
\]
so that
\[
\begin{bmatrix}
c_0^n \\
\ensuremath{\vdots} \\
c_{n-1}^n \end{bmatrix} = Q_n^w \begin{bmatrix} f(x_1) \\ \ensuremath{\vdots} \\ f(x_n) \end{bmatrix}.
\]
Note that if $p(x) = [q_0(x) | \ensuremath{\cdots} | q_{n-1}(x)] \ensuremath{\bm{\c}}$ then
\[
\begin{bmatrix}
p(x_1) \\
\ensuremath{\vdots} \\
p(x_n)
\end{bmatrix} = V_n \ensuremath{\bm{\c}}
\]
But we see that (similar to the Fourier case)
\[
Q_n^w V_n = \begin{bmatrix} \ensuremath{\Sigma}_n^w[q_0 q_0] & \ensuremath{\cdots} & \ensuremath{\Sigma}_n^w[q_0 q_{n-1}]\\
                \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
                \ensuremath{\Sigma}_n^w[q_{n-1} q_0] & \ensuremath{\cdots} & \ensuremath{\Sigma}_n^w[q_{n-1} q_{n-1}]
                \end{bmatrix} = I
\]
and hence $V_n^{-1} = Q_n^w$ and we have
\[
f_n(x_j) = [q_0(x_j) | \ensuremath{\cdots} | q_{n-1}(x_j)] Q_n^w \Vectt[f(x_1),\ensuremath{\vdots},f(x_n)] = \ensuremath{\bm{\e}}_j^\ensuremath{\top} V_n  Q_n^w \Vectt[f(x_1),\ensuremath{\vdots},f(x_n)] = f(x_j).
\]
\ensuremath{\QED}

\begin{example}[Chebyshev expansions]  Consider the construction of Gaussian quadrature associated with the Chebyshev weight for $n = 3$.  To determine the weights we need we compute
\[
w_j^{-1} = \ensuremath{\alpha}_j^2 = q_0(x_j)^2 + q_1(x_j)^2 + q_2(x_j)^2 = 
{1 \over \ensuremath{\pi}} + {2 \over \ensuremath{\pi}} x_j^2 + {2 \over \ensuremath{\pi}} (2x_j^2-1)^2
\]
We can check each case and deduce that $w_j = \ensuremath{\pi}/3$. Thus we recover the interpolatory quadrature rule. Further, we can construct the transform
\begin{align*}
Q_3^w &= \begin{bmatrix}
w_1 q_0(x_1) & w_2 q_0(x_2) & w_3 q_0(x_3) \\
w_1 q_1(x_1) & w_2 q_1(x_2) & w_3 q_1(x_3) \\
w_1 q_2(x_1) & w_2 q_2(x_2) & w_3 q_2(x_3) 
\end{bmatrix}\\
&= {\ensuremath{\pi} \over 3} \begin{bmatrix} 1/\sqrt{\ensuremath{\pi}} & 1/\sqrt{\ensuremath{\pi}} & 1/\sqrt{\ensuremath{\pi}} \\
                                x_1\sqrt{2/\ensuremath{\pi}} & x_2\sqrt{2/\ensuremath{\pi}} & x_3\sqrt{2/\ensuremath{\pi}} \\
                                (2x_1^2-1)\sqrt{2/\ensuremath{\pi}} &(2x_2^2-1)\sqrt{2/\ensuremath{\pi}} & (2x_3^2-1)\sqrt{2/\ensuremath{\pi}}
                                \end{bmatrix} \\
                                &= 
                                {\sqrt{\ensuremath{\pi}} \over 3} \begin{bmatrix} 1 & 1 & 1 \\
                                \sqrt{6}/2 & 0 & -\sqrt{6}/2 \\
                                1/\sqrt{2} &-\sqrt{2} & 1/\sqrt{2}
                                \end{bmatrix}
\end{align*}
We can use this to expand a polynomial, e.g. $x^2$:
\[
Q_3^w \begin{bmatrix}
x_1^2 \\
x_2^2 \\
x_3^2 
\end{bmatrix} = {\sqrt{\ensuremath{\pi}} \over 3} 
\begin{bmatrix} 1 & 1 & 1 \\
\sqrt{6}/2 & 0 & -\sqrt{6}/2 \\
1/\sqrt{2} &-\sqrt{2} & 1/\sqrt{2}
\end{bmatrix} 
\begin{bmatrix} 3/4 \\ 0 \\ 3/4 \end{bmatrix} =
\begin{bmatrix}
{\sqrt{\ensuremath{\pi}} / 2} \\
0 \\
{\sqrt{\ensuremath{\pi}} / (2\sqrt{2})}
\end{bmatrix}
\]
In other words:
\[
x^2 = {\sqrt \ensuremath{\pi} \over 2} q_0(x) + {\sqrt \ensuremath{\pi} \over 2\sqrt 2} q_2(x) = {1 \over 2} T_0(x) + {1 \over 2} T_2(x)
\]
which can be easily confirmed. \end{example}

\begin{corollary}[Gaussian quadrature is interpolatory] Gaussian quadrature is an interpolatory quadrature rule with the interpolation points equal to the roots of $q_n$:
\[
\ensuremath{\Sigma}_n^w[f] = \ensuremath{\int}_a^b f_n(x) w(x) {\rm d}x.
\]
\end{corollary}
\textbf{Proof} We want to show that its the same as integrating the interpolatory polynomial:
\[
\int_a^b f_n(x) w(x) {\rm d}x = {1 \over q_0(x)} \sum_{k=0}^{n-1} c_k^n \int_a^b q_k(x) q_0(x) w(x) {\rm d}x
= {c_0^n \over q_0} = \ensuremath{\Sigma}_n^w[f].
\]
\ensuremath{\QED}

A consequence of being an interpolatory quadrature rule is that it is exact for all polynomials of degree $n-1$. The \emph{miracle} of Gaussian quadrature is it is exact for twice as many!

\begin{theorem}[Exactness of Gauss quadrature] If $p(x)$ is a degree $2n-1$ polynomial then Gauss quadrature is exact:
\[
\ensuremath{\int}_a^b p(x)w(x) {\rm d}x = \ensuremath{\Sigma}_n^w[p].
\]
\end{theorem}
\textbf{Proof} Using polynomial division algorithm (e.g. by matching terms) we can write
\[
p(x) = q_n(x) s(x) + r(x)
\]
where $s$ and $r$ are degree $n-1$ and $q_n(x)$ is the degree $n$ orthonormal polynomial. Because Gauss quadrature is interpolatory we know that it is exact for degree $n-1$ polynomials, in particular:
\[
\ensuremath{\Sigma}_n^w[r] = \ensuremath{\int}_a^b r(x) w(x) {\rm d}x.
\]
But then we find that
\begin{align*}
\ensuremath{\Sigma}_n^w[p] &= \underbrace{\ensuremath{\Sigma}_n^w[q_n s]}_{\hbox{$0$ since evaluating $q_n$ at zeros}} + \ensuremath{\Sigma}_n^w[r] = \ensuremath{\int}_a^b r(x) w(x) {\rm d}x\\
&= \underbrace{\ensuremath{\int}_a^b q_n(x)s(x) w(x) {\rm d}x}_{\hbox{$0$ since $s$ is degree $<n$}}  + \ensuremath{\int}_a^b r(x) w(x) {\rm d}x \\
&= \ensuremath{\int}_a^b p(x)w(x) {\rm d}x.
\end{align*}
\ensuremath{\QED}

\begin{example}[Double exactness] Let's look at an example in completeness for $n = 3$ with uniform weight on $[-1,1]$. From the 3-term recurrence for Legendre polynomials we get the multiplication matrix
\[
x [P_0(x) | P_1(x) | \ensuremath{\cdots} ] = [P_0(x) | P_1(x) | \ensuremath{\cdots} ] \underbrace{\begin{bmatrix} 0 & 1/3\\
                                1 & 0 & 2/5 \\
                                    &2/3 & 0 & 3/7 \\
                                    && 3/5 & 0 & \ensuremath{\ddots} \\
                                    &&& \ensuremath{\ddots} & \ensuremath{\ddots}
                                    \end{bmatrix}}_X
\]
From this we deduce that
\meeq{
P_0(x) = 1 \ccr
P_1(x) = x \ccr
P_2(x) = 3/2 x P_1(x) - P_0(x)/2 = {3x^2 \over 2} - {1 \over 2} \ccr
P_3(x) = 5/3 x P_2(x) - 2P_1(x)/3 = {5 x^2 \over 2} - {3 x \over 2}.
}
The roots of $P_3(x)$ are
\[
x_1,x_2,x_3 = -\sqrt{3/5}, 0, \sqrt{3/5}.
\]
We know the first orthonormal polynomial is $q_0(x) = 1/\sqrt{2}$, i.e., $k_0 = 1/\sqrt{2}$. We write
\[
[q_0(x) | q_1(x) | \ensuremath{\cdots} ] = [P_0(x) | P_1(x) | \ensuremath{\cdots} ] \underbrace{\begin{bmatrix} 1/\sqrt{2} \\ & k_1 \\ && k_2 \\ &&&\ensuremath{\ddots} \end{bmatrix}}_K
\]
Thus from
\[
x [q_0(x) | q_1(x) | \ensuremath{\cdots} ] = [q_0(x) | q_1(x) | \ensuremath{\cdots} ] \underbrace{K^{-1} X K}_J
\]
we find that
\[
J = \begin{bmatrix} 0 & \sqrt{2}k_1/3 \\
                                1/(\sqrt{2}k_1) & 0 & 2k_2/(5k_1) \\
                                    &2k_1/(3k_2) & 0 & 3k_3/(7k_2) \\
                                    && 3k_2/(5k_3) & 0 & \ensuremath{\ddots} \\
                                    &&& \ensuremath{\ddots} & \ensuremath{\ddots}
                                    \end{bmatrix}
\]
For this to be symmetric we find
\meeq{
k_1 = \sqrt{3/2} \ccr
k_2 = \sqrt{10k_1^2/6} = \sqrt{5/2} \ccr
k_3 = \sqrt{21k_2^2/15} = \sqrt{21/6}
}
We thus get the quadrature weights
\meeq{
w_1 = \ensuremath{\alpha}_1^{-2} = {1 \over q_0(x_1)^2 + q_1(x_1)^2 + q_2(x_1)^2} = {1 \over 1/2 + (3/2) \ensuremath{\times} (3/5) + (5/2) \ensuremath{\times} (4/25)} = {5 \over 9} \ccr
w_2 = \ensuremath{\alpha}_2^{-2} = {1 \over q_0(x_2)^2 + q_1(x_2)^2 + q_2(x_2)^2} = {1 \over 1/2 + (5/2) \ensuremath{\times} (1/4)} = {8 \over 9} \ccr
w_3 = w_1 = {5 \over 9}.
}
Thus our Gauss\ensuremath{\endash}Legendre quadrature formula is
\[
\ensuremath{\Sigma}_3^w[f] = {5 \over 9} f(-\sqrt{3/5}) + {8 \over 9} f(0) + {5 \over 9} f(\sqrt{3/5}).
\]
We are exact for all polynomials up to degree $2n-1 = 5$:
\meeq{
\ensuremath{\Sigma}_3^w[1] = {5 \over 9} + {8 \over 9} + {5 \over 9} = 2 \ccr
\ensuremath{\Sigma}_3^w[x] = -{5 \over 9} \sqrt{3/5} + {5 \over 9} \sqrt{3/5} = 0 \ccr
\ensuremath{\Sigma}_3^w[x^2] = {5 \over 9} {3 \over 5} + {5 \over 9} {3 \over 5} = {2 \over 3} \ccr
\ensuremath{\Sigma}_3^w[x^3] = -{5 \over 9} (3/5)^{3/2} + {5 \over 9} (3/5)^{3/2} = 0 \ccr
\ensuremath{\Sigma}_3^w[x^4] = {5 \over 9} {9 \over 25} + {5 \over 9} {9 \over 25} = {2 \over 5} \ccr
\ensuremath{\Sigma}_3^w[x^5] = 0.
}
But the next integral is wrong:
\[
\ensuremath{\Sigma}_3^w[x^6] = {5 \over 9} {27 \over 125} + {5 \over 9} {27 \over 125} = {6 \over 25} \ensuremath{\neq} {2 \over 7} = \ensuremath{\int}_{-1}^1 x^6 {\rm d}x.
\]
\end{example}

Going beyond polynomials,  Gaussian quadrature achieves faster than algebraic convergence for any smooth function.  If the function is analytic in a neighbourhood of the support of the interval this is in fact exponential convergence, far exceeding the convergence rate observed for rectangular and Trapezium rules.  This is a beautiful example of more sophisticated mathematics leading to powerful numerical methods. 



