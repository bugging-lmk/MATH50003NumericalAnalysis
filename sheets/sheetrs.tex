\documentclass[12pt,a4paper]{article}

\usepackage[a4paper,text={16.5cm,25.2cm},centering]{geometry}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage[usenames,dvipsnames]{xcolor}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.2ex}




\hypersetup
       {   pdfauthor = {  },
           pdftitle={  },
           colorlinks=TRUE,
           linkcolor=black,
           citecolor=blue,
           urlcolor=blue
       }




\usepackage{upquote}
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    upquote=true,
    breaklines=true,
    breakindent=0pt,
    keepspaces=true,
    showspaces=false,
    columns=fullflexible,
    showtabs=false,
    showstringspaces=false,
    escapeinside={(*@}{@*)},
    extendedchars=true,
}
\newcommand{\HLJLt}[1]{#1}
\newcommand{\HLJLw}[1]{#1}
\newcommand{\HLJLe}[1]{#1}
\newcommand{\HLJLeB}[1]{#1}
\newcommand{\HLJLo}[1]{#1}
\newcommand{\HLJLk}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkc}[1]{\textcolor[RGB]{59,151,46}{\textit{#1}}}
\newcommand{\HLJLkd}[1]{\textcolor[RGB]{214,102,97}{\textit{#1}}}
\newcommand{\HLJLkn}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkp}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkr}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLkt}[1]{\textcolor[RGB]{148,91,176}{\textbf{#1}}}
\newcommand{\HLJLn}[1]{#1}
\newcommand{\HLJLna}[1]{#1}
\newcommand{\HLJLnb}[1]{#1}
\newcommand{\HLJLnbp}[1]{#1}
\newcommand{\HLJLnc}[1]{#1}
\newcommand{\HLJLncB}[1]{#1}
\newcommand{\HLJLnd}[1]{\textcolor[RGB]{214,102,97}{#1}}
\newcommand{\HLJLne}[1]{#1}
\newcommand{\HLJLneB}[1]{#1}
\newcommand{\HLJLnf}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnfm}[1]{\textcolor[RGB]{66,102,213}{#1}}
\newcommand{\HLJLnp}[1]{#1}
\newcommand{\HLJLnl}[1]{#1}
\newcommand{\HLJLnn}[1]{#1}
\newcommand{\HLJLno}[1]{#1}
\newcommand{\HLJLnt}[1]{#1}
\newcommand{\HLJLnv}[1]{#1}
\newcommand{\HLJLnvc}[1]{#1}
\newcommand{\HLJLnvg}[1]{#1}
\newcommand{\HLJLnvi}[1]{#1}
\newcommand{\HLJLnvm}[1]{#1}
\newcommand{\HLJLl}[1]{#1}
\newcommand{\HLJLld}[1]{\textcolor[RGB]{148,91,176}{\textit{#1}}}
\newcommand{\HLJLs}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsa}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsb}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsc}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsd}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsdC}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLse}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLsh}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsi}[1]{#1}
\newcommand{\HLJLso}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLsr}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLss}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLssB}[1]{\textcolor[RGB]{201,61,57}{#1}}
\newcommand{\HLJLnB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnbB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnfB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnh}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLni}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnil}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLnoB}[1]{\textcolor[RGB]{59,151,46}{#1}}
\newcommand{\HLJLoB}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLow}[1]{\textcolor[RGB]{102,102,102}{\textbf{#1}}}
\newcommand{\HLJLp}[1]{#1}
\newcommand{\HLJLc}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLch}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcm}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcp}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcpB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcs}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLcsB}[1]{\textcolor[RGB]{153,153,119}{\textit{#1}}}
\newcommand{\HLJLg}[1]{#1}
\newcommand{\HLJLgd}[1]{#1}
\newcommand{\HLJLge}[1]{#1}
\newcommand{\HLJLgeB}[1]{#1}
\newcommand{\HLJLgh}[1]{#1}
\newcommand{\HLJLgi}[1]{#1}
\newcommand{\HLJLgo}[1]{#1}
\newcommand{\HLJLgp}[1]{#1}
\newcommand{\HLJLgs}[1]{#1}
\newcommand{\HLJLgsB}[1]{#1}
\newcommand{\HLJLgt}[1]{#1}


\def\endash{â€“}
\def\bbD{ {\mathbb D} }
\def\bbZ{ {\mathbb Z} }
\def\bbR{ {\mathbb R} }
\def\bbC{ {\mathbb C} }

\def\x{ {\vc x} }
\def\a{ {\vc a} }
\def\b{ {\vc b} }
\def\e{ {\vc e} }
\def\f{ {\vc f} }
\def\u{ {\vc u} }
\def\v{ {\vc v} }
\def\y{ {\vc y} }
\def\z{ {\vc z} }
\def\w{ {\vc w} }

\def\bt{ {\tilde b} }
\def\ct{ {\tilde c} }
\def\Ut{ {\tilde U} }
\def\Qt{ {\tilde Q} }
\def\Rt{ {\tilde R} }
\def\Xt{ {\tilde X} }
\def\acos{ {\rm acos}\, }

\def\red#1{ {\color{red} #1} }
\def\blue#1{ {\color{blue} #1} }
\def\green#1{ {\color{ForestGreen} #1} }
\def\magenta#1{ {\color{magenta} #1} }

\input{somacros}

\begin{document}



\textbf{Numerical Analysis MATH50003 (2024\ensuremath{\endash}25) Revision Sheet}

\textbf{Problem 1(a)} State which real number is represented by an IEEE 16-bit floating point number (with $\ensuremath{\sigma} = 15, Q = 5$, and $S = 10$) with bits
\[
{\tt 1\ 01000\ 0000000001}
\]
\textbf{SOLUTION} The sign bit is 1 so the answer is negative. The exponent bits correspond to
\[
q = 2^3 = 8
\]
The significand is
\[
(1.0000000001)_2 = 1 + 2^{-10}
\]
So this represents
\[
-2^{8-\ensuremath{\sigma}} (1 + 2^{-10}) = - 2^{-7} (1 + 2^{-10})
\]
\textbf{END}

\textbf{Problem 1(b)}  How are the following real numbers rounded to the nearest $F_{16}$?
\[
1/2, 1/2 + 2^{-12}, 3 + 2^{-9} + 2^{-10}, 3 + 2^{-10} + 2^{-11}.
\]
\textbf{SOLUTION} $1/2$ is already a float. We have
\[
1/2 + 2^{-12} = (0.100000000001)_2 = 2^{-1} (1.00000000001)_2
\]
This is exactly at the midpoint so is rounded down so the last bit is 0, that is, it is rounded to $1/2$.  Next we have
\[
3 + 2^{-9}  + 2^{-10} = (11.0000000011)_2 = 2(1.10000000011)_2.
\]
This time we are are exactly at the midpoint but we round up so the last bit is 0 giving us
\[
2(1.100000001)_2 = 3 + 2^{-8}.
\]
Finally,
\[
3 + 2^{-10} + 2^{-11} = 2(1.100000000011)_2
\]
This we round up since we are above the midpoint giving us
\[
2(1.1000000001)_2 = 3 + 2^{-9}.
\]
\textbf{END}

\textbf{Problem 2(a)} Consider a Lower triangular matrix with floating point entries:
\[
L = \begin{bmatrix}
\ensuremath{\ell}_{11} \\
 \ensuremath{\ell}_{21} & \ensuremath{\ell}_{22} \\
 \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\ddots} \\
 \ensuremath{\ell}_{n1} & \ensuremath{\cdots} & \ensuremath{\ell}_{n,n-1} & \ensuremath{\ell}_{nn}
 \end{bmatrix} \ensuremath{\in} F_{\ensuremath{\sigma},Q,S}^{n \ensuremath{\times} n}
\]
and a vector $\ensuremath{\bm{\x}} \in F_{\ensuremath{\sigma},Q,S}^{n}$, where $F_{\ensuremath{\sigma},Q,S}$ is a set of floating-point numbers. Denoting matrix-vector multiplication implemented using floating point arithmetic as
\[
\ensuremath{\bm{\b}} := {\tt lowermul}(L,\ensuremath{\bm{\x}})
\]
express the entries $b_k := {\bf e}_k^\ensuremath{\top} \ensuremath{\bm{\b}}$  in terms of $\ensuremath{\ell}_{kj}$ and $x_k := {\bf e}_k^\ensuremath{\top} \ensuremath{\bm{\x}}$,  using rounded floating-point operations $\ensuremath{\oplus}$ and $\ensuremath{\otimes}$.

\textbf{SOLUTION}
\[
b_k = \ensuremath{\bigoplus}_{j=1}^k (\ensuremath{\ell}_{kj} \ensuremath{\otimes} x_j)
\]
\textbf{END}

\textbf{Problem 2(b)} Assuming all operations involve normal floating numbers, show that your approximation has the form
\[
L \ensuremath{\bm{\x}} = {\tt lowermul}(L, \ensuremath{\bm{\x}}) + \ensuremath{\bm{\epsilon}}
\]
where, for $\ensuremath{\epsilon}_{\rm m}$ denoting machine epsilon and $E_{n,\ensuremath{\epsilon}}:= {n \ensuremath{\epsilon} \over 1-n\ensuremath{\epsilon}}$ and assuming $n \ensuremath{\epsilon}_{\rm m} < 2$,
\[
\| \ensuremath{\bm{\epsilon}} \|_1 \ensuremath{\leq}   2E_{n,\ensuremath{\epsilon}_{\rm m}/2}   \|L\|_1 \| \ensuremath{\bm{\x}} \|_1.
\]
Here we use  the matrix norm $\| A \|_1 := \max_j \ensuremath{\sum}_{k=1}^n |a_{kj}|$ and the vector norm $\| \ensuremath{\bm{\x}} \|_1 := \ensuremath{\sum}_{k=1}^n |x_k|$. You may use the fact that
\[
x_1 \ensuremath{\oplus} \ensuremath{\cdots} \ensuremath{\oplus} x_n = x_1 +  \ensuremath{\cdots} + x_n + \ensuremath{\sigma}_n
\]
where
\[
|\ensuremath{\sigma}_n| \ensuremath{\leq} \| \ensuremath{\bm{\x}} \|_1 E_{n-1,\ensuremath{\epsilon}_{\rm m}/2}.
\]
\textbf{SOLUTION}

We have
\[
b_k = (\ensuremath{\bigoplus}_{j=1}^k \ensuremath{\ell}_{kj} \ensuremath{\otimes} x_j) =
(\ensuremath{\bigoplus}_{j=1}^k \ensuremath{\ell}_{kj} x_j (1 + \ensuremath{\delta}_j)) =
(\ensuremath{\sum}_{j=1}^k \ensuremath{\ell}_{kj} x_j (1 + \ensuremath{\delta}_j)) + \ensuremath{\sigma}_k
\]
where
\[
|\ensuremath{\sigma}_k| \ensuremath{\leq} M_k E_{k-1,\ensuremath{\epsilon}_{\rm m}/2}
\]
for
\[
M_k := \ensuremath{\sum}_{j=1}^k  |\ensuremath{\ell}_{kj}| |x_j| |1 + \ensuremath{\delta}_j| \ensuremath{\leq} 2 \ensuremath{\sum}_{j=1}^k  |\ensuremath{\ell}_{kj}| |x_j|.
\]
Thus
\[
b_k = \ensuremath{\bm{\e}}_k^\ensuremath{\top} L \ensuremath{\bm{\x}} + \underbrace{\ensuremath{\sum}_{j=1}^{k} \ensuremath{\ell}_{kj} x_j \ensuremath{\delta}_j + \ensuremath{\sigma}_k}_{\ensuremath{\varepsilon}_k}.
\]
where
\[
|\ensuremath{\varepsilon}_k| \ensuremath{\leq} \ensuremath{\sum}_{j=1}^{k} |\ensuremath{\ell}_{kj}| |x_j| (|\ensuremath{\delta}_j| + 2 E_{k-1,\ensuremath{\epsilon}_{\rm m}/2})
\ensuremath{\leq}  2E_{k,\ensuremath{\epsilon}_{\rm m}/2} \ensuremath{\sum}_{j=1}^{k} |\ensuremath{\ell}_{kj}| |x_j|
\]
where we use
\begin{align*}
 (|\ensuremath{\delta}_j| + 2 E_{k-1,\ensuremath{\epsilon}_{\rm m}/2}) &\ensuremath{\leq} {\ensuremath{\epsilon}_{\rm m} \over 2} + 2 {(k-1) {\ensuremath{\epsilon}_{\rm m} / 2} \over 1-(k-1){\ensuremath{\epsilon}_{\rm m}/ 2}} \cr
 &= {\ensuremath{\epsilon}_{\rm m}/2 - (k-1)\ensuremath{\epsilon}_{\rm m}^2/4 +  2(k-1) {\ensuremath{\epsilon}_{\rm m} / 2} \over 1-(k-1){\ensuremath{\epsilon}_{\rm m}/ 2}} \cr
 &\ensuremath{\leq} {2k {\ensuremath{\epsilon}_{\rm m} / 2} \over 1-k{\ensuremath{\epsilon}_{\rm m}/ 2}} = 2E_{k,\ensuremath{\epsilon}_{\rm m}/ 2}.
\end{align*}
We then have using $E_{k,\ensuremath{\epsilon}_{\rm m}/ 2} \ensuremath{\leq} E_{n,\ensuremath{\epsilon}_{\rm m}/ 2}$,
\meeq{
\| \ensuremath{\bm{\epsilon}} \|_1 = \ensuremath{\sum}_{k=1}^n |\ensuremath{\varepsilon}_k| \ensuremath{\leq} 2E_{n,\ensuremath{\epsilon}_{\rm m}/2} \ensuremath{\sum}_{k=1}^n \ensuremath{\sum}_{j=1}^k |\ensuremath{\ell}_{kj}| |x_j | \ccr
=  2E_{n,\ensuremath{\epsilon}_{\rm m}/2} \ensuremath{\sum}_{j=1}^n  |x_j | \ensuremath{\sum}_{k=1}^{n-j+1} |\ensuremath{\ell}_{kj}| \ensuremath{\leq} 2E_{n,\ensuremath{\epsilon}_{\rm m}/2}  \ensuremath{\sum}_{j=1}^n  |x_j | \|L\|_1 \ccr
= 2E_{n,\ensuremath{\epsilon}_{\rm m}/2} \|L\|_1 \| \ensuremath{\bm{\x}}\|_1.
}
\textbf{END}

\textbf{Problem 3} What is the dual extension of square-roots? I.e. what should $\sqrt{a + b \ensuremath{\epsilon}}$ equal assuming $a > 0$?

\textbf{SOLUTION}
\[
\sqrt{a + b \ensuremath{\epsilon}} = \sqrt{a} + {b \over 2 \sqrt{a}}  \ensuremath{\epsilon}
\]
\textbf{END}

\textbf{Problem 4} Use the Cholesky factorisation to determine whether the following matrix is symmetric positive definite:
\[
\begin{bmatrix} 2 & 2 & 1  \\
2 & 3 & 2\\
1 & 2 & 2
\end{bmatrix}
\]
\textbf{SOLUTION}

Here $\ensuremath{\alpha}_1 = 2$ and $\ensuremath{\bm{\v}} = [2,1]$ giving us
\begin{align*}
A_2 &= \begin{bmatrix}
3&2\\
2&2
\end{bmatrix}-{1 \over 2} \begin{bmatrix} 2 \\ 1 \end{bmatrix}\begin{bmatrix} 2 & 1 \end{bmatrix}\\
&=
\begin{bmatrix}
1&1\\
1&3/2
\end{bmatrix}
\end{align*}
Thus $\ensuremath{\alpha}_2 = 1$ and $\ensuremath{\bm{\v}} = [1]$ giving us
\begin{align*}
A_3 &= [3/2 - 1] = [1/2]
\end{align*}
As $\ensuremath{\alpha}_3 = 1/2 > 0$ we know a Cholesky decomposition exists hence $A$ is SPD. In particular we have computed $A = LL^\ensuremath{\top}$ where
\[
L = \begin{bmatrix}
\sqrt{2} \\
\sqrt{2} & 1 \\
1/\sqrt{2} & 1 & 1/\sqrt{2}
\end{bmatrix}
\]
\textbf{END}

\textbf{Problem 5} Use reflections to determine the entries of an orthogonal matrix $Q$ such that
\[
Q \begin{bmatrix} 2 \\ 1 \\ 2 \end{bmatrix} =  \begin{bmatrix} -3 \\ 0 \\ 0 \end{bmatrix}.
\]
\textbf{SOLUTION}
\begin{align*}
\ensuremath{\bm{\x}} &:= [2,1,2], \| \ensuremath{\bm{\x}} \| = 3\\
\ensuremath{\bm{\y}} &:= \|\ensuremath{\bm{\x}}\| \ensuremath{\bm{\e}}_1 + \ensuremath{\bm{\x}} = [5,1,2], \| \ensuremath{\bm{\y}} \| = \sqrt{30} \\
\ensuremath{\bm{\w}} &:= \ensuremath{\bm{\y}} / \| \ensuremath{\bm{\y}} \| = [5,1,2] /  \sqrt{30} \\
Q &:= I - 2\ensuremath{\bm{\w}} \ensuremath{\bm{\w}}^\ensuremath{\top} = I - {1 \over 15} \begin{bmatrix}5 \\ 1 \\ 2 \end{bmatrix} [5\ 1\ 2] = I - {1 \over 15} \begin{bmatrix} 25 & 5 & 10 \\5 & 1 & 2 \\ 10 & 2 & 4 \end{bmatrix} \\
&= {1 \over 15} \begin{bmatrix} -10 & -5 & -10 \\ -5 & 14 & -2 \\ -10 & -2 & 11 \end{bmatrix}
\end{align*}
\textbf{END}

\textbf{Problem 6} For the function $f(\ensuremath{\theta}) = \sin 3 \ensuremath{\theta}$, state explicit formulae for its Fourier coefficients
\[
\hat f_k := {1 \over 2\ensuremath{\pi}} \int_0^{2\ensuremath{\pi}} f(\ensuremath{\theta}) {\rm e}^{-{\rm i} k \ensuremath{\theta}} {\rm d}\ensuremath{\theta}
\]
and  their discrete approximation:
\[
\hat f_k^n := {1 \over n} \sum_{j=0}^{n-1} f(\ensuremath{\theta}_j) {\rm e}^{-{\rm i} k \ensuremath{\theta}_j}.
\]
for \emph{all} integers $k$, $n = 1,2,\ensuremath{\ldots}$, where $\ensuremath{\theta}_j = 2\ensuremath{\pi} j/n$.

\textbf{SOLUTION}

We have
\[
f(\ensuremath{\theta}) = \sin 3 \ensuremath{\theta} = { \exp(3 i \ensuremath{\theta}) \over 2 i} -  { \exp(-3 i \ensuremath{\theta}) \over 2 i}
\]
hence $\hat f_3 = 1/(2i)$, $\hat f_{-3} = -1/(2i)$ and $\hat f_k = 0$ otherwise. Thus we have:
\begin{align*}
\hat f_k^1 &= \sum_{k=-\ensuremath{\infty}}^\ensuremath{\infty} \hat f_k = \hat f_{-3} + \hat f_3 = 0, \\
\hat f_{2k}^2 &= 0, \hat f_{2k+1}^2 = \hat f_{-3} + \hat f_3 = 0, \\
\hat f_{3k}^3 &= \hat f_{-3} + \hat f_3 = 0, \hat f_{3k+1}^3 = \hat f_{3k-1}^3 = 0, \\
\hat f_{4k}^4 &= \hat f_{4k+2}^4 = 0, \hat f_{4k+1}^4 = \hat f_{-3} = -1/(2i), \hat f_{4k+3}^4 = \hat f_{3} = 1/(2i) \\
\hat f_{5k}^5 &= \hat f_{5k+1}^5 = \hat f_{5k+4}^5,  \hat f_{5k+2}^5 = \hat f_{-3} = -1/(2i),  \hat f_{5k+3}^5 = \hat f_{3} = 1/(2i), \\
\hat f_{6k}^6 &= \hat f_{6k+1}^6 = \hat f_{6k+2}^6 = \hat f_{6k+4}^6 = \hat f_{6k+5}^6,  \hat f_{6k+3}^5 = \hat f_{-3} + \hat f_{3} = 0
\end{align*}
For $n > 6$ we have
\[
\hat f_{-3+nk}^n =  \hat f_{-3} = -{1 \over 2i},\hat f_{3+nk}^n =  \hat f_{3} = {1 \over 2i}
\]
and all other $\hat f_k^n = 0$.

\textbf{END}

\textbf{Problem 7} Consider orthogonal polynomials
\[
H_n(x) = 2^n x^n + O (x^{n-1})
\]
as $x \ensuremath{\rightarrow} \ensuremath{\infty}$ and $n = 0, 1, 2, \ensuremath{\ldots}$,  orthogonal with respect to the inner product
\[
\langle f, g \rangle = \int_{-\ensuremath{\infty}}^\ensuremath{\infty} f(x) g(x) w(x) {\rm d}x, \qquad w(x) = \exp(-x^2)
\]
Construct $H_0(x)$, $H_1(x)$, $H_2(x)$ and hence show that $H_3(x) = 8x^3-12x$. You may use without proof the formulae
\[
\int_{-\ensuremath{\infty}}^\ensuremath{\infty} w(x) {\rm d}x = \sqrt{\ensuremath{\pi}}, \int_{-\ensuremath{\infty}}^\ensuremath{\infty} x^2 w(x) {\rm d}x = \sqrt{\ensuremath{\pi}}/2,
\int_{-\ensuremath{\infty}}^\ensuremath{\infty} x^4 w(x) {\rm d}x = 3\sqrt{\ensuremath{\pi}}/4.
\]
\textbf{SOLUTION}

Because $w(x) = w(-x)$ we know that $a_k$ is zero. We further know that $H_0(x) = 1$ with $\|H_0\|^2 = \sqrt{\ensuremath{\pi}}$  and $H_1(x) = 2x$ with
\[
\| H_1 \|^2 = 4 \ensuremath{\int}_{-\ensuremath{\infty}}^\ensuremath{\infty} x^2 w(x) {\rm d}x = 2 \sqrt{\ensuremath{\pi}}.
\]
We have
\[
 x H_1(x) = c_0 H_0(x) + H_2(x)/2
\]
where
\[
c_0 = {\ensuremath{\langle} x H_1(x), H_0(x) \ensuremath{\rangle} \over \|H_0\|^2 } = {\sqrt{\ensuremath{\pi}}  \over \sqrt{\ensuremath{\pi}}} = 1
\]
Hence $H_2(x) = 2 x H_1(x) - H_0(x) = 4x^2-2$, which satisfies
\[
\|H_2\|^2 = 16 \ensuremath{\int}_{-\ensuremath{\infty}}^\ensuremath{\infty} x^4 w(x) {\rm d } x - 16\ensuremath{\int}_{-\ensuremath{\infty}}^\ensuremath{\infty} x^2 w(x) {\rm d}x + 4 \ensuremath{\int}_{-\ensuremath{\infty}}^\ensuremath{\infty}  w(x) {\rm d}x =
(12 -8 + 4) \sqrt{\ensuremath{\pi}} = 8 \sqrt{\ensuremath{\pi}}.
\]
We further have
\[
\ensuremath{\langle} x H_2(x), H_1(x) \ensuremath{\rangle} =  \ensuremath{\int}_{-\ensuremath{\infty}}^\ensuremath{\infty} (8x^4 - 4 x^2) w(x) {\rm d} x = (6 -2) \sqrt{\ensuremath{\pi}} = 4 \sqrt{\ensuremath{\pi}}
\]
Finally we have
\[
 x H_2(x) = c_1 H_1(x) + H_3(x)/2
\]
where
\[
c_1 = {\ensuremath{\langle} x H_2(x), H_1(x) \ensuremath{\rangle} \over \|H_1\|^2 } = { 4 \sqrt{\ensuremath{\pi}}  \over 2 \sqrt{\ensuremath{\pi}}} = 2
\]
Hence
\[
H_3(x) = 2x H_2(x) - 4 H_1(x) = 8x^3 - 12x.
\]
\textbf{END}

\textbf{Problem 8(a)} Derive the 3-point Gauss quadrature formula
\[
\int_{-\ensuremath{\infty}}^\ensuremath{\infty} f(x) \exp(-x^2) {\rm d}x \ensuremath{\approx} w_1 f(x_1) + w_2 f(x_2) + w_3 f(x_3)
\]
with analytic expressions for $x_j$ and $w_j$.

\textbf{SOLUTION}

We know $x_k$ are the roots of $H_3(x) = 8x^3 - 12x$ hence we have $x_2 = 0$ and the other roots satisfy
\[
2x^2 - 3 = 0,
\]
i.e., $x_1 = -\sqrt{3/2}$ and $x_2 = \sqrt{3/2}$. To deduce the weights the easiest approach is to use Lagrange interpolation. An alternative is to orthonormalise. Note the Jacobi matrix satisfies
\[
x [H_0 | H_1 | H_2 | H_3 | \ensuremath{\ldots}] = [H_0 | H_1 | H_2 | H_3 | \ensuremath{\ldots}] \underbrace{\begin{bmatrix} 0 & 1  \\
                                                                    1/2 & 0 & 2 \\
                                                                       & 1/2 & 0 &  \ensuremath{\ddots}\\
                                                                          && 1/2 & \ensuremath{\ddots} \\
                                                                                &&& \ensuremath{\ddots} \end{bmatrix}}_X
\]
To find $q_k = d_k H_k$, orthonormalised versions of Hermite, we need to choose $d_k$ to symmetrise $X$, that is for $D = {\rm diag}(d_0,d_1,\ensuremath{\ldots})$ we have
\[
x [q_0 | q_1 | \ensuremath{\ldots}] = x [H_0 | H_1 | \ensuremath{\ldots}] D = [H_0 | H_1 | \ensuremath{\ldots}] X D = [q_0 | q_1 | \ensuremath{\ldots}] D^{-1} X D
\]
where
\[
D^{-1} X D = \begin{bmatrix}                                                 0 & d_1/d_0  \\
                                                                    d_0/(2d_1) & 0 & 2d_2/d_1 \\
                                                                    & d_1/(2d_2) & 0 &  \ensuremath{\ddots}\\
                                                                        && d_2/(2d_3) & \ensuremath{\ddots} \\
                                                                     &&& \ensuremath{\ddots} \end{bmatrix}
\]
Note $d_0 = 1/\sqrt{\ensuremath{\int}_{-\ensuremath{\infty}}^\ensuremath{\infty} \exp(-x^2) {\rm d} x } = 1/\ensuremath{\pi}^{1/4}$ then we have
\begin{align*}
d_0^2 &= 2 d_1^2 \ensuremath{\Rightarrow} d_1 = 1/(\sqrt{2} \ensuremath{\pi}^{1/4}) \\
d_1^2 &= 4 d_2^2 \ensuremath{\Rightarrow} d_2 = 1/(2\sqrt{2} \ensuremath{\pi}^{1/4})
\end{align*}
We thus have
\meeq{
w_1 = {1 \over q_0(-\sqrt{3/2})^2 + q_1(-\sqrt{3/2})^2 + q_2(-\sqrt{3/2})^2} =
{1 \over d_0^2 + 4d_1^2 (3/2) + d_2^2 (6 - 2)^2} = {\sqrt{\ensuremath{\pi}} \over 6} \ccr
w_2 = {1 \over q_0(0)^2 + q_1(0)^2 + q_2(0)^2} =
{1 \over d_0^2 + d_2^2 (2)^2} = {2\sqrt{\ensuremath{\pi}} \over  3} \ccr
w_3 = w_1 = {\sqrt{\ensuremath{\pi}} \over 6}.
}
\textbf{END}

\textbf{Problem 8(b)} Compute the 2-point and 3-point Gaussian quadrature rules associated with $w(x) = 1$ on $[-1,1]$.

\textbf{SOLUTION}

For the weights $w(x) = 1$, the orthogonal polynomials of degree $\ensuremath{\leq} 3$ are the Legendre polynomials,
\begin{align*}
	P_0(x) = 1, \\
	P_1(x) = x, \\
	P_2(x) = \frac{1}{2}(3x^2  - 1), \\
	P_3(x) = \frac{1}{2}(5x^3 - 3x)
\end{align*}
which can be found from, e.g, the Rodriguez formula or by direct construction. We can normalise each to get $q_j(x) = P_j(x)/\|P_j\|$, with $\|P_j\|^2 = \int_{-1}^1 P_j^2 dx$. This gives,
\begin{align*}
	q_0(x) &= \frac{1}{\sqrt{2}}, \\
	q_1(x) &= \sqrt{\frac{3}{2}}x, \\
	q_2(x) &= \sqrt{\frac{5}{8}}(3x^2  - 1), \\
	q_3(x) &= \sqrt{\frac{7}{8}}(5x^3 - 3x).
\end{align*}
For the first part we use the roots of $P_2(x)$ which are $\ensuremath{\bm{\x}} = \left\{\ensuremath{\pm} \frac{1}{\sqrt{3}}\right\}$. The weights are,
\[
w_j = \frac{1}{\ensuremath{\alpha}_j^2} = \frac{1}{q_0(x_j)^2 + q_1(x_j)^2} = \frac{1}{\frac{1}{2}+\frac{3}{2}x_j^2},
\]
where $\ensuremath{\alpha}_j$ is the same as in III.6 Lemma 2, so that,
\[
w_1 = w_2 = 1,
\]
and the Gaussian Quadrature rule is,
\[
\ensuremath{\Sigma}_2^w[f] = f\left(-\frac{1}{\sqrt{3}}\right) + f\left(\frac{1}{\sqrt{3}}\right)
\]
For the second part, we use the roots of $P_3(x)$ which are $\ensuremath{\bm{\x}} = \left\{0, \ensuremath{\pm} \sqrt{\frac{3}{5}} \right\}$. The weights are then,
\[
w_j = \frac{1}{\ensuremath{\alpha}_j^2} = \frac{1}{q_0(x_j)^2 + q_1(x_j)^2 + q_2(x_j)^2} = \frac{1}{\frac{9}{8} -\frac{9}{4}x_j^2 + \frac{45}{8}x_j^4 }
\]
Giving us,
\begin{align*}
	w_1 = w_3 = \frac{1}{\frac{9}{8} - \frac{9}{4}\frac{3}{5} + \frac{45}{8}\frac{9}{25}} &= \frac{5}{9} \\
	w_2 &= \frac{8}{9}
\end{align*}
Then the Gaussian Quadrature rule is,
\[
\ensuremath{\Sigma}_3^w[f] = \frac{1}{9} \left[5f\left(-\sqrt\frac{3}{5}\right) +8f(0) + 5f\left(\sqrt\frac{3}{5}\right) \right]
\]
\textbf{END}

\textbf{Problem 9} Solve Problem 4(b) from PS8 using \textbf{Lemma 12 (discrete orthogonality)} with $w(x) = 1/\sqrt{1-x^2}$ on $[-1,1]$. That is, use the connection of $T_n(x)$ with $\cos n \ensuremath{\theta}$ to show that the Discrete Cosine Transform
\[
C_n := \begin{bmatrix}
\sqrt{1/n} \\
 & \sqrt{2/n} \\
 && \ensuremath{\ddots} \\
 &&& \sqrt{2/n}
 \end{bmatrix}
\begin{bmatrix}
    1 & \ensuremath{\cdots} & 1\\
    \cos \ensuremath{\theta}_1 & \ensuremath{\cdots} & \cos \ensuremath{\theta}_n \\
    \ensuremath{\vdots} & \ensuremath{\ddots} & \ensuremath{\vdots} \\
    \cos (n-1)\ensuremath{\theta}_1 & \ensuremath{\cdots} & \cos (n-1)\ensuremath{\theta}_n
\end{bmatrix}
\]
for $\ensuremath{\theta}_j = \ensuremath{\pi}(j-1/2)/n$ is an orthogonal matrix.

\textbf{SOLUTION}

Our goal is to show that $C_n C_n^\ensuremath{\top} = I$. By  Lemma 12 (Discrete Orthogonality) and PS10 Q5, we have,
\begin{align*}
	\ensuremath{\Sigma}_{n}^w[q_lq_m] = \frac{\ensuremath{\pi}}{n}\sum_{j=1}^n q_l(x_j)q_m(x_j) = \ensuremath{\delta}_{lm}.
\end{align*}
where for the weight $w(x) = \frac{1}{\sqrt{1-x^2}}$ we have the orthonormal polynomials $q_0(x_j) = \frac{1}{\sqrt{\ensuremath{\pi}}}$, $q_k(x_j) = \sqrt{\frac{2}{\ensuremath{\pi}}}\cos(k \ensuremath{\theta}_j).$ Thus we have:
\meeq{
\ensuremath{\bm{\e}}_1^\ensuremath{\top} C_n C_n^\ensuremath{\top} \ensuremath{\bm{\e}}_1 = \sqrt{1/n} [1,1,\ensuremath{\ldots},1] \begin{bmatrix}1 \\ \ensuremath{\vdots} \\ 1 \end{bmatrix}  \sqrt{1/n} = {1 \over n} \ensuremath{\sum}_{j=1}^n 1 = 1 \ccr
\ensuremath{\bm{\e}}_k^\ensuremath{\top} C_n C_n^\ensuremath{\top} \ensuremath{\bm{\e}}_1 = \ensuremath{\bm{\e}}_1^\ensuremath{\top} C_n C_n^\ensuremath{\top} \ensuremath{\bm{\e}}_k = \sqrt{1/n} [1,1,\ensuremath{\ldots},1] \begin{bmatrix}\cos (k-1) \ensuremath{\theta}_1 \\ \ensuremath{\vdots} \\ \cos (k-1) \ensuremath{\theta}_n \end{bmatrix}  \sqrt{2/n} \ccr
=
 {1 \over n} \ensuremath{\pi} \ensuremath{\sum}_{\ensuremath{\ell}=1}^n q_k(x_\ensuremath{\ell})q_0(x_\ensuremath{\ell}) = 0 \ccr
 \ensuremath{\bm{\e}}_k^\ensuremath{\top} C_n C_n^\ensuremath{\top} \ensuremath{\bm{\e}}_j = \sqrt{2/n} [\cos (k-1) \ensuremath{\theta}_1, \ensuremath{\ldots} , \cos (k-1) \ensuremath{\theta}_n] \begin{bmatrix}\cos (j-1) \ensuremath{\theta}_1 \\ \ensuremath{\vdots} \\ \cos (j-1) \ensuremath{\theta}_n \end{bmatrix}  \sqrt{2/n} \ccr 
 =
 {\ensuremath{\pi} \over n} \ensuremath{\sum}_{\ensuremath{\ell}=1}^n q_k(x_\ensuremath{\ell})q_j(x_\ensuremath{\ell}) = \ensuremath{\delta}_{kj}.
}
\textbf{END}



\end{document}